global:
  namespace: monitoring

nodeExporter:
  image:
    repository: registry.js.design/prometheus/node-exporter
    tag: v1.0.1
    imagePullPolicy: IfNotPresent

  resources:
    requests:
      cpu: "0.15"
      memory: "100Mi"

  tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Exists"
      effect: "NoSchedule"

prometheus:
  replicaCount: 1
  image:
    repository: registry.js.design/prometheus/prometheus
    tag: v2.32.1
    imagePullPolicy: IfNotPresent

  service:
    port: 9090
    targetPort: 9090
    type: "NodePort"
    customNodePort: 
      enabled: true
      configure:
        nodePort: 30110

  resources:
    requests:
      cpu: "1"
      memory: "200Mi"
    limits:
      cpu: "2"
      memory: "4Gi"

  storageSpec: {}
    # persistentVolumeClaim:
    #   claimName: my-pvc-auto

  rules:
    prometheus-rules.yml: |
      groups:
      - name: example
        rules:
        - alert: kube-proxy的cpu使用率大于80%
          expr: rate(process_cpu_seconds_total{job=~"kubernetes-kube-proxy"}[1m]) * 100 > 80
          for: 2s
          labels:
            severity: warnning
          annotations:
            description: "{{$labels.instance}}的{{$labels.job}}组件的cpu使用率超过80%"
        - alert: scheduler的cpu使用率大于80%
          expr: rate(process_cpu_seconds_total{job=~"kubernetes-schedule"}[1m]) * 100 > 80
          for: 2s
          labels:
            severity: warnning
          annotations:
            description: "{{$labels.instance}}的{{$labels.job}}组件的cpu使用率超过80%"
        - alert: controller-manager的cpu使用率大于80%
          expr: rate(process_cpu_seconds_total{job=~"kubernetes-controller-manager"}[1m]) * 100 > 80
          for: 2s
          labels:
            severity: warnning
          annotations:
            description: "{{$labels.instance}}的{{$labels.job}}组件的cpu使用率超过80%"
        - alert: apiserver的cpu使用率大于80%
          expr: rate(process_cpu_seconds_total{job=~"kubernetes-apiserver"}[1m]) * 100 > 80
          for: 2s
          labels:
            severity: warnning
          annotations:
            description: "{{$labels.instance}}的{{$labels.job}}组件的cpu使用率超过80%"
        - alert: etcd的cpu使用率大于80%
          expr: rate(process_cpu_seconds_total{job=~"kubernetes-etcd"}[1m]) * 100 > 80
          for: 2s
          labels:
            severity: warnning
          annotations:
            description: "{{$labels.instance}}的{{$labels.job}}组件的cpu使用率超过80%"
        - alert: kube-state-metrics的cpu使用率大于80%
          expr: rate(process_cpu_seconds_total{k8s_app=~"kube-state-metrics"}[1m]) * 100 > 80
          for: 2s
          labels:
            severity: warnning
          annotations:
            description: "{{$labels.instance}}的{{$labels.k8s_app}}组件的cpu使用率超过80%"
            value: "{{ $value }}%"
            threshold: "80%"      
        - alert: coredns的cpu使用率大于80%
          expr: rate(process_cpu_seconds_total{k8s_app=~"kube-dns"}[1m]) * 100 > 80
          for: 2s
          labels:
            severity: warnning
          annotations:
            description: "{{$labels.instance}}的{{$labels.k8s_app}}组件的cpu使用率超过80%"
            value: "{{ $value }}%"
            threshold: "80%"      
        - alert: Endpoint_ready
          expr: kube_endpoint_address_not_ready{namespace=~"kube-system|default"} == 1
          for: 2s
          labels:
            team: admin
          annotations:
            description: "空间{{$labels.namespace}}({{$labels.instance}}): 发现{{$labels.endpoint}}不可用"
            value: "{{ $value }}"
            threshold: "1"
      - name: 物理节点状态-监控告警
        rules:
        - alert: 物理节点cpu使用率
          expr: 100-avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) by(instance)*100 > 90
          for: 2s
          labels:
            severity: ccritical
          annotations:
            summary: "{{ $labels.instance }}cpu使用率过高"
            description: "{{ $labels.instance }}的cpu使用率超过90%,当前使用率[{{ $value }}],需要排查处理" 
        - alert: 物理节点内存使用率
          expr: (node_memory_MemTotal_bytes - (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes)) / node_memory_MemTotal_bytes * 100 > 90
          for: 2s
          labels:
            severity: critical
          annotations:
            summary: "{{ $labels.instance }}内存使用率过高"
            description: "{{ $labels.instance }}的内存使用率超过90%,当前使用率[{{ $value }}],需要排查处理"
        - alert: InstanceDown
          expr: up == 0
          for: 2s
          labels:
            severity: critical
          annotations:   
            summary: "{{ $labels.instance }}: 服务器宕机"
            description: "{{ $labels.instance }}: 服务器延时超过2分钟"
        - alert: 物理节点磁盘的IO性能
          expr: 100-(avg(irate(node_disk_io_time_seconds_total[1m])) by(instance)* 100) < 60
          for: 2s
          labels:
            severity: critical
          annotations:
            summary: "{{$labels.mountpoint}} 流入磁盘IO使用率过高! "
            description: "{{$labels.mountpoint }} 流入磁盘IO大于60%(目前使用:{{$value}})"
        - alert: 磁盘容量
          expr: 100-(node_filesystem_free_bytes{fstype=~"ext4|xfs"}/node_filesystem_size_bytes {fstype=~"ext4|xfs"}*100) > 80
          for: 2s
          labels:
            severity: critical
          annotations:
            summary: "{{$labels.mountpoint}} 磁盘分区使用率过高！"
            description: "{{$labels.mountpoint }} 磁盘分区使用大于80%(目前使用:{{$value}}%)"


grafana: 
  image:
    repository: registry.js.design/prometheus/grafana
    tag: 8.0.6
    imagePullPolicy: IfNotPresent

  resources:
    requests:
      cpu: "200m"
      memory: "500Mi"
    limits:
      cpu: "1"
      memory: "2G"

  service: 
    port: 3000
    targetPort: 3000
    type: NodePort
    customNodePort: 
      enabled: true
      configure:
        nodePort: 30114

  # 权限
  permissions:
    imageRepository: registry.js.design/library/busybox
    imageTag: latest
    imagePullPolicy: IfNotPresent

  storageSpec: {}
    # persistentVolumeClaim:
    #   claimName: my-pvc-auto

kubeStateMetrics:
  image:
    repository: registry.cn-shenzhen.aliyuncs.com/starsl/kube-state-metrics
    tag: v2.2.1
    imagePullPolicy: IfNotPresent

  nodeSelector:
    beta.kubernetes.io/os: linux

  livenessProbe:
    enabled: true 
    configure:
      httpGet:
        path: /healthz
        port: 8080
      initialDelaySeconds: 5
      timeoutSeconds: 5

  readinessProbe:
    enabled: true
    configure:
      httpGet:
        path: /
        port: 8081
      initialDelaySeconds: 5
      timeoutSeconds: 5

alertmanager: 
  replicaCount: 1

  image: 
    repository: registry.js.design/prometheus/alertmanager
    tag: v0.22.2
    imagePullPolicy: IfNotPresent

  readinessProbe:
    enabled: true
    configure:
      httpGet:
        path: /#/status
        port: 9093
      initialDelaySeconds: 30
      timeoutSeconds: 30
    
  resources:
    limits:
      cpu: 200m
      memory: 200Mi
    requests:
      cpu: 5m
      memory: 40Mi

  service: 
    port: 9093
    targetPort: 9093
    type: "NodePort"
    customNodePort: 
      enabled: true
      configure:
        nodePort: 30111

  prometheusalertWebHook:
    feishu:
      enabled: true
      url: https://open.feishu.cn/open-apis/bot/v2/hook/2ff32dbc-6797-4d75-xxxxxxxxx

  storageSpec: {}
    # volumeClaimTemplates:
    # - metadata:
    #     name: storage-volume
    #     namespace: monitoring
    #   spec:
    #     storageClassName: managed-nfs-storage
    #     accessModes: [ "ReadWriteOnce" ]
    #     resources:
    #       requests:
    #         storage: 10Gi

prometheusAlert:
  replicaCount: 1
  image:
    repository: registry.js.design/prometheus/prometheus-alert
    tag: latest
    imagePullPolicy: IfNotPresent

  service: 
    port: 8080
    targetPort: 8080
    type: "NodePort"
    customNodePort: 
      enabled: true
      configure:
        nodePort: 30112

  resources:
    limits:
      cpu: 200m
      memory: 200Mi
    requests:
      cpu: 5m
      memory: 40Mi

  storageSpec: {}
    # persistentVolumeClaim:
    #   claimName: my-pvc-auto

pushgateway:
  replicaCount: 1
  image:
    repository: registry.js.design/prometheus/pushgateway
    tag: latest
    imagePullPolicy: IfNotPresent

  service: 
    port: 9091
    targetPort: 9091
    type: "NodePort"
    customNodePort: 
      enabled: true
      configure:
        nodePort: 30113

  resources:
    limits:
      cpu: 500m
      memory: 500Mi
    requests:
      cpu: 5m
      memory: 40Mi