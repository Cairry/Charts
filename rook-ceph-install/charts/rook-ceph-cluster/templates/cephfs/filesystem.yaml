#################################################################################################################
# Create a filesystem with settings with replication enabled for a production environment.
# A minimum of 3 OSDs on different nodes are required in this example.
#  kubectl create -f filesystem.yaml
#################################################################################################################

{{- with .Values.filesystem }}
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: cephfs
  namespace: {{ $.Release.Namespace }}
spec:
  # The metadata pool spec. Must use replication.
  metadataPool:
    replicated:
{{- with .MetadataPoolSize }}
      size: {{ . }}
{{- else }}
      size: 3
{{- end }}
  # The list of data pool specs. Can use replication or erasure coding.
  dataPools:
    - failureDomain: host
      replicated:
{{- with .dataPoolsSize }}
        size: {{ . }}
{{- else }}
        size: 3
{{- end }}
        # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
        # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
        #targetSizeRatio: .5
  # Whether to preserve metadata and data pools on filesystem deletion
  preservePoolsOnDelete: true
  # The metadata service (mds) configuration
  metadataServer:
    # The number of active MDS instances
    activeCount: 1
    # Whether each active MDS instance will have an active standby with a warm metadata cache for faster failover.
    # If false, standbys will be available, but will not have a warm cache.
{{- with .activeStandby }}
    activeStandby: {{ . }}
{{- else }}
    activeStandby: true
{{- end }}
    # The affinity rules to apply to the mds deployment
    placement:
    {{- with .placement }}
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: {{ .nodeSelectorTermsMatchExpressions.key }}
              operator: In
              values:
              {{- range .nodeSelectorTermsMatchExpressions.values }}
              - {{ . }}
              {{- end }}
    {{- end }}
    #  nodeAffinity:
    #    requiredDuringSchedulingIgnoredDuringExecution:
    #      nodeSelectorTerms:
    #      - matchExpressions:
    #        - key: role
    #          operator: In
    #          values:
    #          - mds-node
    #  tolerations:
    #  - key: mds-node
    #    operator: Exists
    #  podAffinity:
    {{- with .antiAffinity }}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - rook-ceph-mds
            topologyKey: kubernetes.io/hostname
      {{- else }}
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - rook-ceph-mds
            topologyKey: kubernetes.io/hostname
      {{- end }}
    # A key/value list of annotations
    annotations:
    #  key: value
    resources:
    {{- with .resources }}
{{ toYaml . | indent 6 }}
    {{- end }}
    # The requests and limits set here, allow the filesystem MDS Pod(s) to use half of one CPU core and 1 gigabyte of memory
    #  limits:
    #    cpu: "500m"
    #    memory: "1024Mi"
    #  requests:
    #    cpu: "500m"
    #    memory: "1024Mi"
    # priorityClassName: my-priority-class
{{- end }}
