#################################################################################################################
# Create a storage class with a pool that sets replication for a production environment.
# A minimum of 3 nodes with OSDs are required in this example since the default failureDomain is host.
#  kubectl create -f storageclass.yaml
#################################################################################################################

apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: {{ $.Release.Namespace }}
Labels:
  chart: "{{ .Chart.Name }}-{{ .Chart.Version | replace "+" "_" }}"
spec:
  failureDomain: host
  replicated:
{{- with .Values.rbdStorageClass.CephBlockPool.size }}
    size: {{ . }}
{{- else }}
    size: 3
{{- end }}
---
{{- if semverCompare ">=1.13.0-0" .Capabilities.KubeVersion.GitVersion }}
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-ceph-block
provisioner: {{ $.Release.Namespace }}.rbd.csi.ceph.com
parameters:
    # clusterID is the namespace where the rook cluster is running
    # If you change this namespace, also change the namespace below where the secret namespaces are defined
    clusterID: {{ $.Release.Namespace }}

    # If you want to use erasure coded pool with RBD, you need to create
    # two pools. one erasure coded and one replicated.
    # You need to specify the replicated pool here in the `pool` parameter, it is
    # used for the metadata of the images.
    # The erasure coded pool must be set as the `dataPool` parameter below.
    #dataPool: ec-data-pool
    pool: replicapool

    # RBD image format. Defaults to "2".
    imageFormat: "2"

    # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
    imageFeatures: layering

    # The secrets contain Ceph admin credentials. These are generated automatically by the operator
    # in the same namespace as the cluster.
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: {{ $.Release.Namespace }}
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: {{ $.Release.Namespace }}
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: {{ $.Release.Namespace }}
    # Specify the filesystem type of the volume. If not specified, csi-provisioner
    # will set default as `ext4`.
    csi.storage.k8s.io/fstype: ext4
# uncomment the following to use rbd-nbd as mounter on supported nodes
#mounter: rbd-nbd
allowVolumeExpansion: true
reclaimPolicy: {{ .Values.rbdStorageClass.reclaimPolicy }}

{{- else if semverCompare "<1.13.0-0" .Capabilities.KubeVersion.GitVersion }}
#################################################################################################################
# Create a storage class with a pool that sets replication for a production environment.
# A minimum of 3 nodes with OSDs are required in this example since the default failureDomain is host.
#  kubectl create -f storageclass.yaml
#################################################################################################################

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: ceph.rook.io/block
# Works for Kubernetes 1.14+
allowVolumeExpansion: true
parameters:
  blockPool: replicapool
  # Specify the namespace of the rook cluster from which to create volumes.
  # If not specified, it will use `rook` as the default namespace of the cluster.
  # This is also the namespace where the cluster will be
  clusterNamespace: {{ $.Release.Namespace }}
  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.
  fstype: xfs
  # (Optional) Specify an existing Ceph user that will be used for mounting storage with this StorageClass.
  #mountUser: user1
  # (Optional) Specify an existing Kubernetes secret name containing just one key holding the Ceph user secret.
  # The secret must exist in each namespace(s) where the storage will be consumed.
  #mountSecret: ceph-user1-secret
reclaimPolicy: {{ .Values.rbdStorageClass.reclaimPolicy }}
{{- end }}
